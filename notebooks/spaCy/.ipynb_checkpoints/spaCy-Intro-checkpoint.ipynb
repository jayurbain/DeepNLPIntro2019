{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaCy Intro\n",
    "\n",
    "Jay Urbain, PhD\n",
    "\n",
    "References and credits:  \n",
    "https://spacy.io/      \n",
    "\n",
    "spaCy is an open-source natural language processing library written in Python. It levarages the SciPy ecosystem. The API is relatively easy to use and is well defined.\n",
    "\n",
    "spaCy is very efficient and is designed for production use. The spaCy open-source team quickly encorprates new NLP models, and the spaCy library interoperates well with other machine learning libraries including TensorFlow, PyTorch, scikit-learn, and Gensim.\n",
    "\n",
    "Some other options for NLP libraires include:  \n",
    "- [NLTK](https://www.nltk.org/) . Most popular Python NLP library. More difficult to use, does not typically have top performing models. Several add-on packages. \n",
    "- [Stanford CoreNLP](https://stanfordnlp.github.io/CoreNLP/) . Written in Java, has Python API library. Several state-of-the-art models including dependency parsing.\n",
    "- [Gensim](https://github.com/RaRe-Technologies/gensim) . Written in Python, best for unsupervised learning NLP tasks like topic modeling and word vectors.\n",
    "- Many more ...\n",
    "\n",
    "Here's a quick comparison of the models from the spaCy website. More can be found here: [Spacy Facts and Figures](https://spacy.io/usage/facts-figures)       \n",
    "\n",
    "<img src=\"spacy_nltk_corenlp_comparison.png\" width=\"400px\"/>                                                 \n",
    "\n",
    "The goal of this notebooks is to provide and introduction and getting started guide for using spaCy for baseline NLP tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installation (not required for Colab) \n",
    "\n",
    "Pip:\n",
    "`python -m venv .env\n",
    "source .env/bin/activate\n",
    "pip install spacy`\n",
    "\n",
    "Conda:\n",
    "`conda install -c conda-forge spacy`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check spacy version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Models & Languages\n",
    "\n",
    "spaCy’s models can be installed as Python packages. This means that they’re a component of your application, just like any other module. They’re versioned and can be defined as a dependency in your `requirements.txt`. Models can be installed from a download URL or a local directory, manually or via pip. Their data can be located anywhere on your file system.\n",
    "\n",
    "Install model:\n",
    "`python -m spacy download en_core_web_sm`\n",
    "\n",
    "External URL (very helpful for requirements.txt during deployment):\n",
    "`pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz`\n",
    "\n",
    "or local file:\n",
    "`pip install /Users/you/en_core_web_sm-2.1.0.tar.gz`\n",
    "\n",
    "requirements.txt model format:   \n",
    "`spacy>=2.0.0,<3.0.0\n",
    "https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can explore the meta-data for a spacy model prior to loading it. The description and pipeline fields identify the NLP functionaity provided by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy.info(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLP object and tokenization\n",
    "\n",
    "At the center of spaCy is the object containing the processing pipeline which is intantiated by loading a model. Usually referenced by the variable \"nlp\".\n",
    "\n",
    "In the following example, when we load the \"en_core_web_sm\" model we instantiate an `nlp` pipeline with the functionality provided by that model. See the meta-data above.\n",
    "\n",
    "When we process a sentence through the pipeline, spaCy creates in a document (Doc) object. The Doc lets you access information about the text in a structured way. In our exaple, we can access each token of `text` from the tokenization provided by the pipeline.\n",
    "\n",
    "Pipeline annotations can be accessed like any other Python sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\") # load model package \"en_core_web_sm\"\n",
    "doc = nlp(u\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "for token in doc:\n",
    "    print(token.text)\n",
    "    \n",
    "doc[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. On your own\n",
    "\n",
    "# Import spacy\n",
    "import _____\n",
    "\n",
    "# Create the nlp object\n",
    "nlp = ____\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"This is a sentence.\")\n",
    "\n",
    "# Print the document text\n",
    "print(____.text)\n",
    "\n",
    "# Print text of each token\n",
    "for token in doc:\n",
    "    -----.----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solutions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also create a slice of tokens within doc, or characters within a token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc.text)\n",
    "print( doc[1:6] )\n",
    "print( doc[2].text[1:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lexical attributes\n",
    "\n",
    "You can access token text to make lexical (text) comparisons.\n",
    "\n",
    "Check whether the next token’s text attribute is a percent sign ”%“.\n",
    "\n",
    "The `like_num` token attribute can be used to check if a token is a number.\n",
    "\n",
    "The index of the next token in the doc is token.i + 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Import the English language class\n",
    "import spacy\n",
    "\n",
    "# Create the nlp object\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(\n",
    "    \"In 1990, more than 60% of people in East Asia were in extreme poverty. \"\n",
    "    \"Now less than 4% are.\"\n",
    ")\n",
    "\n",
    "# Iterate over the tokens in the doc\n",
    "for token in doc:\n",
    "    # Check if the token resembles a number\n",
    "    if ____.____:\n",
    "        # Get the next token in the document\n",
    "        next_token = ____[____]\n",
    "        # Check if the next token's text equals '%'\n",
    "        if next_token.____ == \"%\":\n",
    "            print(\"Percentage found:\", token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solutions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistical models\n",
    "\n",
    "spaCy's statistical models allow you to predict linguistic attributes in context\n",
    "- Part-of-speech tags  \n",
    "- Syntactic dependencies  \n",
    "- Named entities  \n",
    "\n",
    "Models are trained on labeled example texts and can be updated with more examples to fine-tune predictions.\n",
    "\n",
    "The `en_core_web_sm` package which we have already loaded, is a small English model that supports all core spaCycapabilities and is trained on web text.\n",
    "\n",
    "The `spacy.load` method loads a model package by name and returns an nlp object.\n",
    "\n",
    "The package provides the binary weights that enable spaCy to make predictions.\n",
    "\n",
    "It also includes the vocabulary, and meta information to tell spaCy which language class to use and how to configure the processing pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the English language class\n",
    "import spacy\n",
    "\n",
    "# Create the nlp object\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Predicting parts-of-speech*\n",
    "\n",
    "Part of speech tagging can be helpful for several downstream NLP tasks, e.g., noun-chunking for candidate entities, named entity recognition, and sentence parsing.\n",
    "\n",
    "For each token in the Doc, we can print the text and the \"pos underscore\" attribute, the predicted part-of-speech tag.\n",
    "\n",
    "In spaCy, attributes that return strings usually end with an underscore – attributes without the underscore return an ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the small English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"Chronic obstructive pulmonary disease (COPD) is a chronic inflammatory lung disease that causes obstructed airflow from the lungs.\")\n",
    "\n",
    "# Iterate over the tokens\n",
    "for token in doc:\n",
    "    # Print the text and the predicted part-of-speech tag\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Predicting syntactic dependencies*\n",
    "\n",
    "Using a dependency parser, we can predict how words in a sentence are related. This is especially helpful for extracting entity attributes and identify entity relations.\n",
    "\n",
    "In addition to the part-of-speech tags, we can also predict how the words are related. For example, whether a word is the subject of the sentence or an object.\n",
    "\n",
    "In the example below, `Chronic, obstructive, pulmonary, and pulmonary` are all modifiers of the NOUN `disease`. And together identify a distinct entity NOUN phrase.\n",
    "\n",
    "`Chronic obstructive pulmonary disease` is related to `chronic inflammatory lung disease` with the verb `is`. Also called an `is-a` or `type` relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_, token.head.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Dependency label scheme*\n",
    "\n",
    "<img src=\"spacy_dependency_label_scheme.png\" width=\"500px\"/>\n",
    "\n",
    "The pronoun \"She\" is a nominal subject attached to the verb \"ate\".\n",
    "\n",
    "The noun \"pizza\" is a direct object attached to the verb \"ate\". It is eaten by the subject, \"she\".\n",
    "\n",
    "The determiner \"the\", also known as an article, is attached to the noun \"pizza\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Predicting Named Entities*\n",
    "\n",
    "Named entities are \"real world objects\" that are assigned a name, e.g., person, location, organization, or country. The task of identifying named entities in text is typically called *named entity recognition (NER)*.\n",
    "\n",
    "The `doc.ents` property lets you access the named entities predicted by the model.\n",
    "\n",
    "It returns an iterator of `Span` objects (character positions), so we can print the entity text and the entity label using the \"label underscore\" attribute.\n",
    "\n",
    "In this case, the model is correctly predicting \"Apple\" as an organization, \"U.K.\" as a geopolitical entity and \"$1 billion\" as money.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "# Iterate over the predicted entities\n",
    "for ent in doc.ents:\n",
    "    # Print the entity text and its label\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets try a slightly more complex example.\n",
    "\n",
    "The results aren't too good. The acronymn for LBP for low back pain is labeled as an organization, and location is missed, In fact NER is one of the most common and useful task in NLP. Different domains have different vocabularies and require specially trained NER's.\n",
    "\n",
    "1 - Try a few examples of your own.  \n",
    "2 - Try out a medical named entity recognizer here: https://cis.ctsi.mcw.edu/nlp/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u\"Jay Urbain, is an aging caucasian male suffering from illusions of grandeur and LBP. Jay has been prescribed meloxicam, and venti americano. He lives at 9050 N. Tennyson Dr., Disturbia, WI with his wife Kimberly Urbain.\")\n",
    "\n",
    "# Iterate over the predicted entities\n",
    "for ent in doc.ents:\n",
    "    # Print the entity text and its label\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get definitions of the most common tags and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( 'GPE:', spacy.explain('GPE') )\n",
    "\n",
    "print( 'NNP:', spacy.explain('NNP') )\n",
    "\n",
    "print( 'dobj:', spacy.explain('dobj') )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting linguistic annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. On your own\n",
    "\n",
    "- Process the text with the nlp object and create a doc.  \n",
    "- For each token, print the token text, the token’s .pos_ (part-of-speech tag) and the token’s .dep_ (dependency label).\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"It’s official: Apple is the first U.S. public company to reach a $1 trillion market value\"\n",
    "\n",
    "# Process the text\n",
    "doc = ____\n",
    "\n",
    "for token in doc:\n",
    "    # Get the token text, part-of-speech tag, and dependency label\n",
    "    token_text = ____.____\n",
    "    token_pos = ____.____\n",
    "    token_dep = ____.____\n",
    "    # This is for formatting only\n",
    "    print(\"{:<12}{:<10}{:<10}\".format(token_text, token_pos, token_dep))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. On your own\n",
    "\n",
    "# Process the text and create a doc object.  \n",
    "# Iterate over the doc.ents and print the entity text and label_ attribute.  \n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"It’s official: Apple is the first U.S. public company to reach a $1 trillion market value\"\n",
    "\n",
    "# Process the text\n",
    "doc = ____\n",
    "\n",
    "# Iterate over the predicted entities\n",
    "for ent in ____.____:\n",
    "    # Print the entity text and its label\n",
    "    print(ent.____, ____.____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solutions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary:  \n",
    "    \n",
    "We covered the core capabilities of spaCy. spaCy has a lot more useful functionality, including tools for annotating your data and a machine learning library for training your own models.\n",
    "\n",
    "spaCy also has a machine learning libary to build custom models for appliations like named entity recognition and text classification.\n",
    "\n",
    "For more information read the documentation and take the tutorials at spaCy: https://spacy.io/ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. On your own\n",
    "\n",
    "# Import the English language class\n",
    "import spacy\n",
    "\n",
    "# Create the nlp object\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"This is a sentence.\")\n",
    "\n",
    "# Print the document text\n",
    "print(doc.text)\n",
    "\n",
    "# Print text of each token\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Import the English language class\n",
    "import spacy\n",
    "\n",
    "# Create the nlp object\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(\n",
    "    \"In 1990, more than 60% of people in East Asia were in extreme poverty. \"\n",
    "    \"Now less than 4% are.\"\n",
    ")\n",
    "\n",
    "# Iterate over the tokens in the doc\n",
    "for token in doc:\n",
    "    # Check if the token resembles a number\n",
    "    if token.like_num:\n",
    "        # Get the next token in the document\n",
    "        next_token = doc[token.i + 1]\n",
    "        # Check if the next token's text equals '%'\n",
    "        if next_token.text == \"%\":\n",
    "            print(\"Percentage found:\", token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. On your own\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"It’s official: Apple is the first U.S. public company to reach a $1 trillion market value\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    # Get the token text, part-of-speech tag and dependency label\n",
    "    token_text = token.text\n",
    "    token_pos = token.pos_\n",
    "    token_dep = token.dep_\n",
    "    # This is for formatting only\n",
    "    print(\"{:<12}{:<10}{:<10}\".format(token_text, token_pos, token_dep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. On your own\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"It’s official: Apple is the first U.S. public company to reach a $1 trillion market value\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Iterate over the predicted entities\n",
    "for ent in doc.ents:\n",
    "    # Print the entity text and its label\n",
    "    print(ent.text, ent.label_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-qa",
   "language": "python",
   "name": "ml-qa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
